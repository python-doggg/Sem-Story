# device
mode: train  # train sample
gpu_ids: [0,1] # gpu ids
batch_size: 1  # batch size each item denotes one story yuan 1
num_workers: 4  # number of workers
num_cpu_cores: -1  # number of cpu cores
seed: 0  # random seed
ckpt_dir: /home/pengjie/ARLDM/save_ckpt/first/ # checkpoint directory
run_name: train_pororo_stage2 # name for this run

# task
dataset: pororo  # pororo flintstones vistsis vistdii
task: continuation  # continuation visualization

# train
init_lr: 1e-5  # initial learning rate
warmup_epochs: 1  # warmup epochs
max_epochs: 50  # max epochs
train_model_file: /home/pengjie/StoryGen_c/stage2_pf_log/1010_0.4/checkpoint_40000/ #/home/pengjie/StoryGen_c/stage1_pf_log/p1/checkpoint_12000/ #/home/share/models/stable-diffusion-v1-5 # model file for resume, none for train from scratch /home/pengjie/StoryGen_c/stage1_pf_log/pororo77/checkpoint_5000/

# sample
test_model_file: /home/pengjie/StoryGen_c/stage2_pf_log/p2/checkpoint_60000/ # /home/pengjie/ARLDM/save_ckpt/first/last.ckpt  # model file for test
calculate_fid: True  # whether to calculate FID scores
scheduler: ddim  # ddim pndm
guidance_scale: 6  # guidance scale
num_inference_steps: 250  # number of inference steps
sample_output_dir: /home/pengjie/StoryGen_c/test_all # output directory

pororo:
  hdf5_file: /mnt/lustre/pengjie/data/h5/pororo.h5
  max_length: 77 # yuan 85
  new_tokens: [ "pororo", "loopy", "eddy", "harry", "poby", "tongtong", "crong", "rody", "petty" ]
  clip_embedding_tokens: 49416
  blip_embedding_tokens: 30530

flintstones:
  hdf5_file: /mnt/lustre/pengjie/data/h5/flintstones.h5
  max_length: 91
  new_tokens: [ "fred", "barney", "wilma", "betty", "pebbles", "dino", "slate" ]
  clip_embedding_tokens: 49412
  blip_embedding_tokens: 30525

logdir: /home/pengjie/StoryGen_c/stage2_pf_log/
validation_sample_logger:
    num_inference_steps: 250 # yuan 40
    guidance_scale: 6 # yuan 7
gradient_accumulation_steps: 10
train_batch_size: 12 # yuan 12
train_steps: 200000 #50000
validation_steps: 500 # 500
checkpointing_steps: 20000
mixed_precision: 'fp16'
learning_rate: 1e-5
val_batch_size: 1
scale_lr: false
lr_scheduler: constant
lr_warmup_steps: 300 # yuan 0
use_8bit_adam: true
adam_beta1: 0.9
adam_beta2: 0.999
adam_weight_decay: 0.01
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
alpha: 0.4